<!DOCTYPE html><html class="appearance-auto" lang="en"><head><meta charset="UTF-8"><title>Coding Fxxker's blog</title><meta name="description" content="矩阵侠"><meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no, initial-scale=1"><!-- Google Analytics --><!-- End Google Analytics -->
<!-- Baidu Analytics --><!-- End Baidu Analytics --><link rel="icon" href="/images/%E5%A4%B4%E5%83%8F.jpg"><link rel="stylesheet" href="/style/common/bulma.css"><link rel="stylesheet" href="/style/base.css"><link rel="stylesheet" href="/style/common/helper.css"><script src="/js/common.js"></script><link rel="stylesheet" href="/style/widget-post-list.css"><meta name="generator" content="Hexo 5.2.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="MatrixMan's blog" type="application/atom+xml">
</head><body class="is-flex is-flex-direction-column"><header class="header-widget is-flex-shrink-0 is-hidden-mobile"><div class="container is-fullhd is-flex is-justify-content-space-between is-align-items-center is-full-height"><section class="is-hidden-mobile is-flex-shrink-0"><h2><a href="/">Coding Fxxker's blog</a></h2></section><h3 class="is-hidden-mobile is-family-serif is-full-height is-flex is-align-items-center is-flex-shrink-0"><span>Categories · python</span></h3><aside class="is-flex-shrink-0"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></aside></div></header><header class="is-flex header-widget is-flex-shrink-0 is-align-items-center is-justify-content-center is-hidden-tablet"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></header><main><article class="post-container is-flex is-justify-content-center section container is-max-widescreen pt-4 px-2"><div class="columns is-variable is-1-tablet is-3-desktop-only is-2-widescreen is-full-width"><section class="column"><article class="post-item-card"><header class="is-relative is-flex"><div class="post-cover-backdrop is-hidden"><img src="预览.jpg" alt="loading.."></div><a class="post-cover-link has-text-centered skeleton" href="/2022/04/18/%E4%BD%BF%E7%94%A8pyautogui%E5%BA%93%E5%AE%9E%E7%8E%B0%E7%BD%91%E9%A1%B5%E8%87%AA%E5%8A%A8%E6%88%AA%E5%8F%96pdf/"><img class="post-cover-img js-img-fadeIn" src="预览.jpg" alt="loading.." data-backdrop="true"></a></header><section class="content post-card-content p-4 pb-5"><header><a href="/tags/%E8%87%AA%E5%8A%A8%E5%8C%96%E5%8A%9E%E5%85%AC"><i class="tag post-item-tag">自动化办公</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2022/04/18/%E4%BD%BF%E7%94%A8pyautogui%E5%BA%93%E5%AE%9E%E7%8E%B0%E7%BD%91%E9%A1%B5%E8%87%AA%E5%8A%A8%E6%88%AA%E5%8F%96pdf/">使用Pyautogui库实现网页自动截取Pdf</a></h2><time class="has-text-grey" datetime="2022-04-18T15:25:29.000Z">2022-04-18</time><p class="is-flex-grow-2 mt-2">引言
又接到老板任务，需要从一个预览pdf的网页强行下载若干pdf, 每本pdf少则80页多则200页。但最反人类的一点是什么呢？ 它网页css配置中采用了限制最大加载策略，每次只能显示10张，所以不能直接用浏览器的打印成pdf。 不是万不得已， 笔者不会采用最蠢方式完成重复性劳动的。尝试了采用爬虫获取， 但无奈前端方面技术太菜， 无法绕过最大显示设置， 即使能绕过， 显示还可能有问题。 无奈只能用最土的方法， 模拟鼠标键盘操作来用截图功能保存jpg，再转pdf。
问题初探
 页面中值得注意的是， 不仅有竖页，也有横页，抽象布局如下  为了视觉上区分方便，这里灰色代表横页， 白色代表竖页，他们在实际中均为白色，以下所述所有颜色均为实际颜色。A、B为两个比较靠近页面顶部的，在一条水平线上的的两个点， 要注意选..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2022/04/18/%E4%BD%BF%E7%94%A8pyautogui%E5%BA%93%E5%AE%9E%E7%8E%B0%E7%BD%91%E9%A1%B5%E8%87%AA%E5%8A%A8%E6%88%AA%E5%8F%96pdf/">Read more</a></section></article><article class="post-item-card"><section class="content post-card-content p-4 pb-5"><header><a href="/tags/%E7%AE%97%E6%B3%95,%E7%AC%94%E8%AF%95,%E6%A6%82%E7%8E%87"><i class="tag post-item-tag">算法,笔试,概率</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2021/09/06/%E8%85%BE%E8%AE%AF%E7%AC%94%E8%AF%95%E7%AE%97%E6%B3%95%E5%8D%B79-5%E7%AC%AC%E4%BA%94%E9%A2%98/">腾讯笔试算法卷9.5第五题</a></h2><time class="has-text-grey" datetime="2021-09-06T05:22:04.000Z">2021-09-06</time><p class="is-flex-grow-2 mt-2">题目：小A在玩一个网络游戏，有一个抽装备环节。装备池总共有n+m件装备， 分别为n件普通装备和m件ssr装备。每次抽中一件ssr级装备，花费2元， 不放回。每次抽中一件普通装备，花费1元， 放回。所有装备抽中的概率相等。问：小A若想抽走所有ssr级装备，所有花费的期望是多少元？ \(\sum_{i=0}^N\int_{a}^{b}g(t,i)\text{d}t\) 这里提供一个大佬的简便思路。 先考虑在n件普通装备，m件神装情况下， 只抽一件神装的花费期望。这里，抽中一件神装的概率为p=m/(m+n)。现在计算抽的 次数的期望。 假若第一次抽中，概率为1/p, 第二次才抽中(第一次放回),概率为1/(1-p)1/p, 第三次才抽中，概率为 1/(1-p)1/(1-p)1/p。。。 这个分布期望就是所谓几何分..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2021/09/06/%E8%85%BE%E8%AE%AF%E7%AC%94%E8%AF%95%E7%AE%97%E6%B3%95%E5%8D%B79-5%E7%AC%AC%E4%BA%94%E9%A2%98/">Read more</a></section></article><article class="post-item-card"><header class="is-relative is-flex"><div class="post-cover-backdrop is-hidden"><img src="显卡.jpg" alt="loading.."></div><a class="post-cover-link has-text-centered skeleton" href="/2021/07/23/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%87%AA%E5%8A%A8%E9%80%89%E6%8B%A9%E6%98%BE%E5%8D%A1/"><img class="post-cover-img js-img-fadeIn" src="显卡.jpg" alt="loading.." data-backdrop="true"></a></header><section class="content post-card-content p-4 pb-5"><header><a href="/tags/%E6%AD%A3%E5%88%99%E5%BC%8F%E5%8C%B9%E9%85%8D"><i class="tag post-item-tag">正则式匹配</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2021/07/23/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%87%AA%E5%8A%A8%E9%80%89%E6%8B%A9%E6%98%BE%E5%8D%A1/">跑深度学习代码时自动选择显卡</a></h2><time class="has-text-grey" datetime="2021-07-22T16:43:08.000Z">2021-07-23</time><p class="is-flex-grow-2 mt-2"> ### 引言 长期以来，笔者在跑深度学习实验时，总会遇到这样一个问题: 究竟该选哪块(or 哪几块)卡？ 因为笔者使用的服务器一共有9块显卡, 有10个人可以用，每个人随机时间段使用。那么很常见的情况就是, 我头一天跑过的代码，第二天再跑的时候运行了几分钟甚至十来分钟后突然报错: OOM Error。。。跟吃shit了一样难受，最后灰溜溜的在终端输入nvidia-smi(我alias了也还是感觉麻烦)，用肉眼分析哪几个剩余显卡可以用。对于使用tensorflow的同学来说，尤是一场灾难，因为tensorflow的前摇时间比pytorch真的长了不知道多少。因此，如果能在每个深度学习代码前，加一个自动检测可用显卡程序，岂不是节约了很多时间。
问题探究
1)try... except...
伪代码
for c..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2021/07/23/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%87%AA%E5%8A%A8%E9%80%89%E6%8B%A9%E6%98%BE%E5%8D%A1/">Read more</a></section></article><article class="post-item-card"><header class="is-relative is-flex"><div class="post-cover-backdrop is-hidden"><img src="爬虫.jpg" alt="loading.."></div><a class="post-cover-link has-text-centered skeleton" href="/2021/07/05/%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BA%E5%85%AC%E5%BC%80%E6%95%B0%E6%8D%AE%E9%9B%86python%E7%88%AC%E8%99%AB/"><img class="post-cover-img js-img-fadeIn" src="爬虫.jpg" alt="loading.." data-backdrop="true"></a></header><section class="content post-card-content p-4 pb-5"><header><a href="/tags/%E7%88%AC%E8%99%AB"><i class="tag post-item-tag">爬虫</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2021/07/05/%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BA%E5%85%AC%E5%BC%80%E6%95%B0%E6%8D%AE%E9%9B%86python%E7%88%AC%E8%99%AB/">零基础快速搭建公开数据集python爬虫</a></h2><time class="has-text-grey" datetime="2021-07-05T12:13:45.000Z">2021-07-05</time><p class="is-flex-grow-2 mt-2"> ### 引言
最近接到老板任务，需要重新下载前段时间下过的一个数据集。这个数据集网站有多反人类呢？我们前段时间两个人断断续续点了三天才下载完。原因是数据集本身是多条的，如果点击合并下载， 系统后台会先把所有的数据压缩成一个文件，再给你这个压缩包的下载链接。一次打包数量稍微多一点点就会打包失败， 即使成功了， 打包的时间爆炸长。一共几千条数据， 大概每次只能点二三十个， 要经历打包-下载-解压-归类， 还得和下一批数据起始位置衔接好， 还要两个人分工好。 第一次下载这个数据集的时候我都快吐了。身为一个具有hacker精神的人， 我不容许自己再这么蠢。 好奇心的驱使下， 花了4个小时零基础终于完成了爬虫代码， 优化了2个人3天的耗时。


几千条数据需要分段勾选



这里就点了十几个，半分钟了打包进度还是0..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2021/07/05/%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BA%E5%85%AC%E5%BC%80%E6%95%B0%E6%8D%AE%E9%9B%86python%E7%88%AC%E8%99%AB/">Read more</a></section></article><section class="paginator is-flex is-justify-content-flex-end is-flex-wrap-wrap mt-5"><span class="page-number current">1</span><a class="page-number" href="/categories/python/page/2/">2</a><a class="extend next" rel="next" href="/categories/python/page/2/"><i class="iconfont icon-next has-text-grey"></i></a></section></section><aside class="column is-hidden-mobile is-4-tablet is-3-widescreen"><div style="position: sticky; top: 50px;"><main class="aside-card-container categories-widget category-page"><h3>Categories</h3><section><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/python/">python</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%BF%90%E7%BB%B4/">服务器运维</a><span class="category-list-count">1</span></li></ul></section></main></div></aside></div></article><script>$claudia.fadeInImage(null, $claudia.blurBackdropImg)

window.addEventListener('resize', $claudia.throttle(function () {
    var images = document.querySelectorAll('.js-img-fadeIn')

    images.forEach($claudia.blurBackdropImg)
}, 150))</script></main><footer class="is-flex is-flex-direction-column is-align-items-center is-flex-shrink-0 is-family-serif"><section class="sns-container"><a title="twitter" target="_blank" rel="noopener nofollow" href="//twitter.com/jmjkx111"><i class="iconfont icon-twitter"></i></a><!-- Github--><a title="github" target="_blank" rel="noopener nofollow" href="//github.com/jmjkx"><i class="iconfont icon-github"></i></a><!-- Ins--><a title="instagram" target="_blank" rel="noopener nofollow" href="//www.instagram.com/jmjkx"><i class="iconfont icon-ins"></i></a><!-- RSS--><a title="rss" target="_blank" rel="noopener nofollow" href="/atom.xml"><i class="iconfont icon-rss"></i></a><!-- 知乎--><a title="zhihu" target="_blank" rel="noopener nofollow" href="//zhihu.com/people/codefxxker"><i class="iconfont icon-zhihu"></i></a><!-- 领英--><!-- 脸书--></section><p><span>Copyright ©</span><span> Coding Fxxker 2022</span></p><div class="is-flex is-justify-content-center is-flex-wrap-wrap"><p>Powered by Hexo &verbar;&nbsp;</p><p class="is-flex is-justify-content-center"><a title="Hexo theme author" target="_blank" rel="noopener" href="//github.com/haojen">Theme by Haojen&nbsp;</a></p><div style="margin-top: 2px"><a class="github-button" title="github-button" target="_blank" rel="noopener" href="https://github.com/haojen/hexo-theme-Claudia" data-color-scheme="no-preference: light; light: light; dark: dark;" data-show-count="true"></a></div></div><div><span></span></div></footer><script async defer src="https://buttons.github.io/buttons.js"></script><script>$claudia.fadeInImage(null, $claudia.blurBackdropImg)

window.addEventListener('resize', $claudia.throttle(function () {
    var images = document.querySelectorAll('.js-img-fadeIn')

    images.forEach($claudia.blurBackdropImg)
}, 150))</script></body></html>